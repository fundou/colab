{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fundou/colab/blob/master/karpathy/istrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer istrain\n",
        "\n",
        "Q: Does a Transformer know if it being trained? This has implications on AI safety.\n",
        "\n",
        "Hypothesis: dropout \"leaks\" the train/eval phase bit."
      ],
      "metadata": {
        "id": "oT5U8GeLzF8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUtAR0bTzBR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repro\n",
        "torch.manual_seed(42);"
      ],
      "metadata": {
        "id": "_mX9k8G81wcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create a toy transformer network doing BCE loss on last token\n",
        "C = 64 # num channels\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super(TinyTransformer, self).__init__()\n",
        "        # random small encoder decoder transformer\n",
        "        self.transformer = nn.Transformer(d_model=C, nhead=4, \n",
        "                       num_encoder_layers=4, num_decoder_layers=4,\n",
        "                       dim_feedforward=C*4, dropout=dropout)\n",
        "        self.fc = nn.Linear(C, 1)\n",
        "    def forward(self, xe, xd):\n",
        "        # forward the transformer\n",
        "        x = self.transformer(xe, xd)\n",
        "        # select the last time step to make the prediction\n",
        "        x = x[:, -1, :]\n",
        "        # forward the classifier\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cz8TPWoEzS_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # training loop\n",
        "    B, T = 8, 4\n",
        "    steps = 300\n",
        "\n",
        "    for n in range(steps):\n",
        "\n",
        "        # zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # phase 1: train mode\n",
        "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
        "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
        "        model.train()\n",
        "        x = model(xe, xd)\n",
        "        y = torch.ones(B, 1) # positive label: we are training\n",
        "        loss = F.binary_cross_entropy_with_logits(x, y)\n",
        "        loss.backward()\n",
        "        if n % 100 == 0 or n == steps-1:\n",
        "            print(f\"{n} loss in phase 1: {loss.item()}\")\n",
        "\n",
        "        # phase 2: eval mode\n",
        "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
        "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
        "        model.eval()\n",
        "        x = model(xe, xd)\n",
        "        y = torch.zeros(B, 1) # negative label: we are not training\n",
        "        loss = F.binary_cross_entropy_with_logits(x, y)\n",
        "        loss.backward()\n",
        "        if n % 100 == 0 or n == steps-1:\n",
        "            print(f\"{n} loss in phase 2: {loss.item()}\")\n",
        "\n",
        "        # update\n",
        "        optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "KTEMpYIdzh8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model):\n",
        "\n",
        "    # evaluate accuracy on some synthetic test data\n",
        "    corrects = []\n",
        "    for test in range(200):\n",
        "        \n",
        "        # dummy input\n",
        "        B, T = 1, 4\n",
        "        xe = torch.randn(B, T, C) # B,T,C for encoder\n",
        "        xd = torch.randn(B, T, C) # B,T,C for decoder\n",
        "\n",
        "        # set network into train/eval phase\n",
        "        phase = test % 2\n",
        "        model.train() if phase == 1 else model.eval()\n",
        "        \n",
        "        # predict mode\n",
        "        x = model(xe, xd)\n",
        "        y = torch.sigmoid(x)\n",
        "        pred = 1 if y.item() > 0.5 else 0\n",
        "        \n",
        "        # print(f\"{test} gt: {phase}, pred: {pred}, correct: {phase == pred}\")\n",
        "        corrects.append(float(phase == pred))\n",
        "\n",
        "    print(f\"test accuracy {torch.tensor(corrects).mean().item()*100}%\")\n"
      ],
      "metadata": {
        "id": "IYmJ5K9o1XFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with dropout > 0.0 this should work, i.e. accuracy >> 50%\n",
        "model = TinyTransformer(dropout=0.2)\n",
        "train_model(model)\n",
        "eval_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R9Zp4RP1Tu4",
        "outputId": "df01185a-0af5-4062-aac1-8ac09fbf3872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss in phase 1: 0.6486659049987793\n",
            "0 loss in phase 2: 1.0286682844161987\n",
            "100 loss in phase 1: 0.005957315675914288\n",
            "100 loss in phase 2: 0.003156071063131094\n",
            "200 loss in phase 1: 0.0032816240563988686\n",
            "200 loss in phase 2: 0.0018672486767172813\n",
            "299 loss in phase 1: 0.001967259682714939\n",
            "299 loss in phase 2: 0.001231701229698956\n",
            "test accuracy 95.49999833106995%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with dropout of 0 this should not work, i.e. accuracy ~= 50%\n",
        "model = TinyTransformer(dropout=0.0)\n",
        "train_model(model)\n",
        "eval_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mqoPJHA0t5m",
        "outputId": "1b663064-cbff-4c2c-8527-703503af51aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss in phase 1: 1.0923291444778442\n",
            "0 loss in phase 2: 0.6770550608634949\n",
            "100 loss in phase 1: 0.6910950541496277\n",
            "100 loss in phase 2: 0.6947798132896423\n",
            "200 loss in phase 1: 0.608734130859375\n",
            "200 loss in phase 2: 0.7696783542633057\n",
            "299 loss in phase 1: 0.6994112730026245\n",
            "299 loss in phase 2: 0.6903160810470581\n",
            "test accuracy 50.49999952316284%\n"
          ]
        }
      ]
    }
  ]
}